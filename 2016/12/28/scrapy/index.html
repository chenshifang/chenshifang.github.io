<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="陈士方">





<title>scrapy框架 | 陈士方的博客</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
    


	
<meta name="generator" content="Hexo 4.2.0"></head>


<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo">
				<a href="/">
				陈士方的博客
				</a>
			</div>
			
			
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">目录</a>
                
                    <a class="menu-item" href="/category">分类</a>
                
                    <a class="menu-item" href="/tag">标签</a>
                
                    <a class="menu-item" href="/about">关于</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>

        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">陈士方的博客</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">目录</a>
                
                    <a class="menu-item" href="/category">分类</a>
                
                    <a class="menu-item" href="/tag">标签</a>
                
                    <a class="menu-item" href="/about">关于</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">scrapy框架</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">陈士方</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">十二月 28, 2016&nbsp;&nbsp;0:00:00</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/spider/">spider</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <p>Scrapy主要包括了以下组件：</p>
<ul>
<li><strong>引擎(Scrapy)</strong><br>用来处理整个系统的数据流处理, 触发事务(框架核心)</li>
<li><strong>调度器(Scheduler)</strong><br>用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址<a id="more"></a>)</li>
<li><strong>下载器(Downloader)</strong><br>用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)</li>
<li><strong>爬虫(Spiders)</strong><br>爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</li>
<li><strong>项目管道(Pipeline)</strong><br>负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</li>
<li><strong>下载器中间件(Downloader Middlewares)</strong><br>位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。</li>
<li><strong>爬虫中间件(Spider Middlewares)</strong><br>介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。</li>
<li><strong>调度中间件(Scheduler Middewares)</strong><br>介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。</li>
</ul>
<p>Scrapy运行流程大概如下：</p>
<ol>
<li><p>引擎从调度器中取出一个链接(URL)用于接下来的抓取</p>
</li>
<li><p>引擎把URL封装成一个请求(Request)传给下载器</p>
</li>
<li><p>下载器把资源下载下来，并封装成应答包(Response)</p>
</li>
<li><p>爬虫解析Response</p>
</li>
<li><p>解析出实体（Item）,则交给实体管道进行进一步的处理</p>
</li>
<li><p>解析出的是链接（URL）,则把URL交给调度器等待抓取</p>
</li>
</ol>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>Linux下的安装(包括mac)</p>
<p>　　pip install scrapy</p>
<p>Windows下的安装</p>
<p>　　a. 下载twisted<br>　　　　<a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted" target="_blank" rel="noopener">http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</a><br>　　b. 安装wheel<br>　　　　pip3 install wheel<br>　　c. 安装twisted<br>　　　　<code>进入下载目录，执行</code> pip3 install Twisted‑18.7.0‑cp36‑cp36m‑win_amd64.whl<br>　　d. 安装pywin32<br>　　　　pip3 install pywin32<br>　　e. 安装scrapy<br>　　　　pip3 install scrapy</p>
<h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a><strong>常用命令</strong></h3><p> 1，在当前目录中创建一个项目文件（类似于Django）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject 项目名称</span><br></pre></td></tr></table></figure>

<p> 2，创建一个爬虫应用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider 爬虫名 域名 (注意：爬虫名尽量不要和项目名重名)</span><br></pre></td></tr></table></figure>

<p> 3，启动爬虫应用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl 爬虫名 </span><br><span class="line">scrapy crawl 爬虫名 --nolog(不打印日志)</span><br></pre></td></tr></table></figure>

<p> 4，查看帮助信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy -help</span><br></pre></td></tr></table></figure>

<p>5，查看版本信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy version -v</span><br></pre></td></tr></table></figure>

<p>6，查看项目里的爬虫</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy list</span><br></pre></td></tr></table></figure>

<p>7，执行一个基准的测试，用来检测爬虫是否安装成功</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy bench</span><br></pre></td></tr></table></figure>

<p>注：命令不全，只是几个常用的；详情见spider中文文档：<a href="http://www.scrapyd.cn/doc/181.html" target="_blank" rel="noopener">http://www.scrapyd.cn/doc/181.html</a></p>
<h3 id="项目结构以及爬虫应用简介"><a href="#项目结构以及爬虫应用简介" class="headerlink" title="项目结构以及爬虫应用简介"></a><strong>项目结构以及爬虫应用简介</strong></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">project_name&#96;&#96;&#x2F;&#96;&#96;  &#96;&#96;scrapy.cfg&#96;&#96;  &#96;&#96;project_name&#96;&#96;&#x2F;&#96;&#96;    &#96;&#96;__init__.py&#96;&#96;    &#96;&#96;items.py&#96;　　　　　　　middlewares.py &#96;    &#96;&#96;pipelines.py&#96;&#96;    &#96;&#96;settings.py&#96;&#96;    &#96;&#96;spiders&#96;&#96;&#x2F;&#96;&#96;      &#96;&#96;__init__.py&#96;&#96;      &#96;&#96;爬虫&#96;&#96;1.py&#96;&#96;      &#96;&#96;爬虫&#96;&#96;2.py&#96;&#96;      &#96;&#96;爬虫&#96;&#96;3.py</span><br></pre></td></tr></table></figure>

<p>文件说明：</p>
<ul>
<li>scrapy.cfg  项目的主配置信息。（真正爬虫相关的配置信息在settings.py文件中）</li>
<li>items.py   设置数据存储模板，用于结构化数据，如：Django的Model</li>
<li>pipelines   数据处理行为，如：一般结构化的数据持久化</li>
<li>settings.py 配置文件，如：递归的层数、并发数，延迟下载等</li>
<li>spiders    爬虫目录，如：创建文件，编写爬虫规则</li>
</ul>
<p><em>注意：一般创建爬虫文件时，以网站域名命名</em></p>
<p>windows系统编码错误时：</p>
<p>解决方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys,io</span><br><span class="line">sys.stdout=io.TextIOWrapper(sys.stdout.buffer,encoding=<span class="string">'gb18030'</span>)</span><br></pre></td></tr></table></figure>



<p>备注：可以在settings.py中配置user-agent</p>
<p>在爬取数据时，可以选择是否往…/robots.txt/发送验证，是否允许爬取，一般设置为False</p>
<p>使用scrapy解析文本内容时，可以使用每个应用中的response.xpath(xxx) 进行数据的解析。</p>
<p>print(response.xpath(…)) 得到的是一个<code>Selector对象。selector对象可以继续xpath进行数据的解析。</code></p>
<p>备注：xpath使用方法：<br>　　1.//+标签 表示从全局的子子孙孙中查找标签   </p>
<p>　　2./+标签  表示从子代中查找标签</p>
<p>　　3.查找带有xxx属性的标签：  标签+[@标签属性=”值”]  </p>
<p>　　4.查找标签的某个属性： /标签/@属性  </p>
<p>　　5.从当前标签中查找时：.//+标签    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">response = HtmlResponse(url=<span class="string">'http://example.com'</span>, body=html,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">hxs = HtmlXPathSelector(response)</span><br><span class="line">print(hxs)   <span class="comment"># selector对象</span></span><br><span class="line">hxs = Selector(response=response).xpath(<span class="string">'//a'</span>)</span><br><span class="line">print(hxs)    <span class="comment">#查找所有的a标签</span></span><br><span class="line">hxs = Selector(response=response).xpath(<span class="string">'//a[2]'</span>)</span><br><span class="line">print(hxs)    <span class="comment">#查找某一个具体的a标签    取第三个a标签</span></span><br><span class="line">hxs = Selector(response=response).xpath(<span class="string">'//a[@id]'</span>)</span><br><span class="line">print(hxs)    <span class="comment">#查找所有含有id属性的a标签</span></span><br><span class="line">hxs = Selector(response=response).xpath(<span class="string">'//a[@id="i1"]'</span>)</span><br><span class="line">print(hxs)    <span class="comment"># 查找含有id=“i1”的a标签</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[@href="link.html"][@id="i1"]')</span></span><br><span class="line"><span class="comment"># print(hxs)   # 查找含有href=‘xxx’并且id=‘xxx’的a标签</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[contains(@href, "link")]')</span></span><br><span class="line"><span class="comment"># print(hxs)   # 查找 href属性值中包含有‘link’的a标签</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[starts-with(@href, "link")]')</span></span><br><span class="line"><span class="comment"># print(hxs)   # 查找 href属性值以‘link’开始的a标签</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[re:test(@id, "i\d+")]')</span></span><br><span class="line"><span class="comment"># print(hxs)   # 正则匹配的用法   匹配id属性的值为数字的a标签</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[re:test(@id, "i\d+")]/text()').extract()</span></span><br><span class="line"><span class="comment"># print(hxs)    # 匹配id属性的值为数字的a标签的文本内容</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//a[re:test(@id, "i\d+")]/@href').extract()</span></span><br><span class="line"><span class="comment"># print(hxs)    #匹配id属性的值为数字的a标签的href属性值</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('/html/body/ul/li/a/@href').extract()</span></span><br><span class="line"><span class="comment"># print(hxs)</span></span><br><span class="line"><span class="comment"># hxs = Selector(response=response).xpath('//body/ul/li/a/@href').extract_first()</span></span><br><span class="line"><span class="comment"># print(hxs)</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># ul_list = Selector(response=response).xpath('//body/ul/li')</span></span><br><span class="line"><span class="comment"># for item in ul_list:</span></span><br><span class="line"><span class="comment">#     v = item.xpath('./a/span')</span></span><br><span class="line"><span class="comment">#     # 或</span></span><br><span class="line"><span class="comment">#     # v = item.xpath('a/span')</span></span><br><span class="line"><span class="comment">#     # 或</span></span><br><span class="line"><span class="comment">#     # v = item.xpath('*/a/span')</span></span><br><span class="line"><span class="comment">#     print(v)</span></span><br></pre></td></tr></table></figure>




<p>备注：xpath中支持正则的使用：  用法 标签+[re:test（@属性值，”正则表达式”）]</p>
<p>　　获取标签的文本内容：  /text()   </p>
<p>　　获取第一个值需要 selector_obj.extract_first()  获取所有的值 selector_obj.extract() 值在一个list中</p>
<h2 id="scrapy的持久化"><a href="#scrapy的持久化" class="headerlink" title="scrapy的持久化"></a>scrapy的持久化</h2><p>scrapy的持久化过程分为四个部分</p>
<p>　　首先，items定义传输的格式，其次，在爬虫应用中yield这个item对象，pipeline收到yield的item对象，进行持久化操作，这个过程中，settings中要进行相应的配置</p>
<p>items.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 规范持久化的格式</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyspiderItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    url=scrapy.Field()</span><br></pre></td></tr></table></figure>

<p>爬虫应用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span>  myspider.items <span class="keyword">import</span> MyspiderItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChoutiSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'chouti'</span></span><br><span class="line">    allowed_domains = [<span class="string">'chouti.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://dig.chouti.com/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># print(response.text)</span></span><br><span class="line">        a_list = response.xpath(<span class="string">'//div[@id="content-list"]//div[@class="part1"]/a[@class="show-content color-chag"]/@href'</span>).extract()</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> a_list:</span><br><span class="line">            <span class="keyword">yield</span> MyspiderItem(url=url)</span><br></pre></td></tr></table></figure>


<p>pipelines.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyspiderPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,file_path)</span>:</span></span><br><span class="line">        self.f = <span class="literal">None</span></span><br><span class="line">        self.file_path = file_path</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls,crawler)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        执行pipeline类时，会先去类中找from_crawler的方法，</span></span><br><span class="line"><span class="string">        如果有，则先执行此方法，并且返回一个当前类的对象，</span></span><br><span class="line"><span class="string">        如果没有，则直接执行初始化方法</span></span><br><span class="line"><span class="string">        :param crawler:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="comment"># 可以进行一些初始化之前的处理，比如：文件的路径配置到settings文件中，方便后期的更改。</span></span><br><span class="line">        file_path = crawler.settings.get(<span class="string">'PACHONG_FILE_PATH'</span>)</span><br><span class="line">        <span class="keyword">return</span> cls(file_path)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        爬虫开始时被调用</span></span><br><span class="line"><span class="string">        :param spider:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.f = open(self.file_path,<span class="string">'w'</span>,encoding=<span class="string">'utf8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        执行持久化的逻辑操作</span></span><br><span class="line"><span class="string">        :param item: 爬虫yield过来的item对象  (一个字典)</span></span><br><span class="line"><span class="string">        :param spider:  爬虫对象</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.f.write(item[<span class="string">'url'</span>]+<span class="string">'\n'</span>)</span><br><span class="line">        self.f.flush()   <span class="comment">#将写入到内存的文件强刷到文件中，防止夯住，不使用此方法会夯住</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        爬虫结束时调用</span></span><br><span class="line"><span class="string">        :param spider: </span></span><br><span class="line"><span class="string">        :return: </span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        self.f.close()</span><br></pre></td></tr></table></figure>


<p>备注：*<em>执行pipeline时，会先找from_crawler方法，这个方法中，我们可以设置一些settings文件中的配置，通过crawler.settings得到一个settings对象（配置文件对象） *</em></p>
<p>　　<strong>执行pipeline中的process_item() 方法进行数据的持久化处理时，如果有多个pipeline（比如：将数据分别写入文件和数据库）时，先执行的pipeline（按照配置文件中数值的大小顺序执行），必须返回一个item对象，否则，后续的pipeline执行时，接收的item为None，无法进行数据的持久化操作，如果只是单纯的对某些数据进行一个持久化的处理，可以通过抛出异常，来阻止当前item对象后续的pipeline执行。抛出异常为：from scrapy.exceptions import DropItem    直接 raise DropItem()</strong></p>
<p>　　<strong>return不返回item对象与抛异常的区别：无返回值或者返回值为None时，后续的pipeline会执行，只是，此时的item为None，而抛出异常，会跳过当前对象后续的pipeline，执行下一个item对象。</strong></p>
<p>settings.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;  </span><br><span class="line">　　<span class="string">'myspider.pipelines.MyspiderPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">　　<span class="string">'xxxxx.pipelines.FilePipeline'</span>: <span class="number">400</span>, &#125; <span class="comment"># 每行后面的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内。</span></span><br></pre></td></tr></table></figure>

<p>备注：数值小的先执行。</p>
<p> 获取所有页面</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span>  myspider.items <span class="keyword">import</span> MyspiderItem</span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> Request</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChoutiSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'chouti'</span></span><br><span class="line">    allowed_domains = [<span class="string">'chouti.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://dig.chouti.com/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        a_list = response.xpath(<span class="string">'//div[@id="content-list"]//div[@class="part1"]/a[@class="show-content color-chag"]/@href'</span>).extract()</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> a_list:</span><br><span class="line">            <span class="keyword">yield</span> MyspiderItem(url=url)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取分页的url</span></span><br><span class="line">        url_list = response.xpath(<span class="string">'//div[@id="dig_lcpage"]//a/@href'</span>).extract()</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> url_list:</span><br><span class="line">            url = <span class="string">'https://dig.chouti.com%s'</span>%url</span><br><span class="line">            <span class="keyword">yield</span> Request(url=url,callback=self.parse)</span><br></pre></td></tr></table></figure>



<p>备注：通过yield 每一个request对象，将所有的页面url添加到调度器中。</p>
<p>　　scrapy框架会默认的将所有的结果进行去重操作。如果不去重，可以在request参数中，设置 dont_filter=True</p>
<p> <em>注意：settings.py中设置DEPTH_LIMIT = 1来指定“递归”的层数  ，这里的层数不是页码数</em></p>
<p> 在生成的每一个爬虫应用中，会有一个起始url，start_urls = [‘<a href="https://dig.chouti.com/&#39;]，这个起始url执行完后会被parse回调函数接收响应结果。那我们如何修改这个回调函数呢？" target="_blank" rel="noopener">https://dig.chouti.com/&#39;]，这个起始url执行完后会被parse回调函数接收响应结果。那我们如何修改这个回调函数呢？</a></p>
<p>　　其实，在每一个爬虫应用继承的父类中，会执行一个方法 start_requests ，这个方法，会将起始的url生成一个request对象，传给调度器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Spider</span><span class="params">(object_ref)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        cls = self.__class__</span><br><span class="line">        <span class="keyword">if</span> method_is_overridden(cls, Spider, <span class="string">'make_requests_from_url'</span>):</span><br><span class="line">            warnings.warn(</span><br><span class="line">                <span class="string">"Spider.make_requests_from_url method is deprecated; it "</span></span><br><span class="line">                <span class="string">"won't be called in future Scrapy releases. Please "</span></span><br><span class="line">                <span class="string">"override Spider.start_requests method instead (see %s.%s)."</span> % (</span><br><span class="line">                    cls.__module__, cls.__name__</span><br><span class="line">                ),</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> url <span class="keyword">in</span> self.start_urls:</span><br><span class="line">                <span class="keyword">yield</span> self.make_requests_from_url(url)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> url <span class="keyword">in</span> self.start_urls:</span><br><span class="line">                <span class="keyword">yield</span> Request(url, dont_filter=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<p>　备注：在执行爬虫应用时，会先执行start_requests方法，所以我们可以重写此方法自定制。</p>
<h2 id="获取响应数据中的cookie"><a href="#获取响应数据中的cookie" class="headerlink" title="获取响应数据中的cookie"></a>获取响应数据中的cookie</h2><p>返回的response中，无法通过 .cookies 获取cookie，只能通过从响应头中获取，但是获取的结果还得需要解析。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;b&#39;Server&#39;: [b&#39;Tengine&#39;], b&#39;Content-Type&#39;: [b&#39;text&#x2F;html; charset&#x3D;UTF-8&#39;], b&#39;Date&#39;: [b&#39;Fri, 20 Jul 2018 13:43:42GMT&#39;], b&#39;Cache-Control&#39;: [b&#39;private&#39;], </span><br><span class="line">b&#39;Content-Language&#39;: [b&#39;en&#39;], </span><br><span class="line">b&#39;Set-Cookie&#39;: [b&#39;gpsd&#x3D;5b05bcae8c6f4a273a53addfc8bbff22; domain&#x3D;chouti.com; path&#x3D;&#x2F;; expires&#x3D;Sun, </span><br><span class="line">19-Aug-2018 13:43:42 GMT&#39;, b&#39;JSESSIONID&#x3D;aaadbrXmUJh2_kvbaysw; path&#x3D;&#x2F;&#39;], </span><br><span class="line">b&#39;Vary&#39;: [b&#39;Accept-Encoding&#39;], </span><br><span class="line">b&#39;Via&#39;: [b&#39;cache15.l2nu29-1[69,0], kunlun9.cn125[73,0]&#39;], b&#39;Timing-Allow-Origin&#39;: [b&#39;*&#39;], </span><br><span class="line">b&#39;Eagleid&#39;: [b&#39;6a78b50915320942226762320e&#39;]&#125;</span><br></pre></td></tr></table></figure>

<p>所以，要通过scrapy封装的方法，将cookie解析出来</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.http.cookies <span class="keyword">import</span> CookieJar</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChoutiSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'chouti'</span></span><br><span class="line">    allowed_domains = [<span class="string">'chouti.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://dig.chouti.com/'</span>]</span><br><span class="line">    cookie_dict = &#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line"></span><br><span class="line">        cookie_jar = CookieJar()</span><br><span class="line">        cookie_jar.extract_cookies(response,response.request)   </span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> cookie_jar._cookies.items():</span><br><span class="line">            <span class="keyword">for</span> i, j <span class="keyword">in</span> v.items():</span><br><span class="line">                <span class="keyword">for</span> m, n <span class="keyword">in</span> j.items():</span><br><span class="line">                    self.cookie_dict[m] = n.value</span><br><span class="line">        print(self.cookie_dict)</span><br></pre></td></tr></table></figure>


<p>备注：CookieJar中封装的内容特别丰富,print(cookie_jar._cookies)  包含很多</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'.chouti.com'</span>: &#123;<span class="string">'/'</span>: &#123;<span class="string">'gpsd'</span>: Cookie(version=<span class="number">0</span>, name=<span class="string">'gpsd'</span>, value=<span class="string">'fcb9b9da7aaede0176d2a88cde8b6adb'</span>, port=<span class="literal">None</span>, port_specified=<span class="literal">False</span>, domain=<span class="string">'.chouti.com'</span>, domain_specified=<span class="literal">True</span>, domain_initial_dot=<span class="literal">False</span>, path=<span class="string">'/'</span>, path_specified=<span class="literal">True</span>, secure=<span class="literal">False</span>, expires=<span class="number">1534688487</span>, discard=<span class="literal">False</span>, comment=<span class="literal">None</span>, comment_url=<span class="literal">None</span>, rest=&#123;&#125;, rfc2109=<span class="literal">False</span>)&#125;&#125;, </span><br><span class="line"><span class="string">'dig.chouti.com'</span>: &#123;<span class="string">'/'</span>: &#123;<span class="string">'JSESSIONID'</span>: Cookie(version=<span class="number">0</span>, name=<span class="string">'JSESSIONID'</span>, value=<span class="string">'aaa4GWMivXwJf6ygMaysw'</span>, port=<span class="literal">None</span>, port_specified=<span class="literal">False</span>, domain=<span class="string">'dig.chouti.com'</span>, domain_specified=<span class="literal">False</span>, domain_initial_dot=<span class="literal">False</span>, path=<span class="string">'/'</span>, path_specified=<span class="literal">True</span>, secure=<span class="literal">False</span>, expires=<span class="literal">None</span>, discard=<span class="literal">True</span>, comment=<span class="literal">None</span>, comment_url=<span class="literal">None</span>, rest=&#123;&#125;, rfc2109=<span class="literal">False</span>)&#125;&#125;&#125;</span><br></pre></td></tr></table></figure>


<p>自动登录抽屉并点赞和取消赞代码示例</p>
<p>备注：爬取过程中的坑：请求头中，一定要携带content-type参数。请求过程中的url不能重复，尤其是和起始url。</p>
<p>我们可以使用urllib中的urlencode帮我们把数据转化为formdata格式的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">ret = &#123;<span class="string">'name'</span>:<span class="string">'xxx'</span>,<span class="string">'age'</span>:<span class="number">18</span>&#125;</span><br><span class="line"></span><br><span class="line">print(urlencode(ret))</span><br></pre></td></tr></table></figure>
        </div>

        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/spider/"># spider</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2016/12/30/VUE/">VUE简介</a>
            
            
            <a class="next" rel="next" href="/2016/12/27/%E6%AD%A3%E5%88%99/">正则表达式</a>
            
        </section>


    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© 陈士方 | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>
			
    </div>
	
<iframe src="https://zhanyuzhang.github.io/lovely-cat/cat.html" frameborder="0" id="catIframe"></iframe>

</body>


<style>
#catIframe {
position: fixed;
width: 400px;
/* padding: 50px 0px; */
top: 80%;
left: 10%;
transform: translate(-68%, -50%);
}

</style>
</html>
